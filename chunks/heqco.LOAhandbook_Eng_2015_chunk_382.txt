effective, coordinated and structured throughout the program; students are meeting key learning outcomes; and investment in assessment pays off in improvement and enhancement of programs. For an example, see Case Study: Washington State University Monitoring Progress in Assessment for Improvement below. Case Study: Washington State University, Monitoring Progress in Assessment for Improvement Context: “The Senate of Washington State University [WSU] instituted its initial program assessment policy in 1990. The 2009 update to the policy specified that “the faculty associated with each undergraduate and graduate degree program will develop a plan for assessing…students about to receive the degree… Departments must be able to demonstrate improvements over time.”11 Examples of Assessment for Improvement: The Office of Assessment of Teaching and Learning provides numerous examples of how WSU programs use assessment results. • “ The School of Economic Sciences changed their curriculum to include a research requirement…after assessments revealed student weaknesses in applied economic and quantitative tools.”12 • “The Human Development program collected supervisors’ rating of student interns in order to…strengthen weaker skill areas. [The curriculum] changes have boosted supervisor ratings.”13 • “The School of Food Science focused on improving students’ skills in communication and time management after surveying alumni and industry employers about WSU graduates on the job.”14 Monitoring Progress in Assessment for Improvement: WSU also uses departmental self-assessment to track the evolution of assessment for improvement processes within programs. The table below highlights how institutions can use a rubric for process maturity to plan support for programs in assessment for improvement.15 In response to the report, the accreditor cited a “noticeable transformation of the culture of assessment”16 at WSU. Holistic Self-Assessment of Program Assessment System Number of Programs 4 (7%) 20 (37%) 20 (37%) 10 (19%) DEVELOPMENT OF BEGINNING: DEVELOPING: REFINING: ESTABLISHED: ASSESSMENT SYSTEM One iteration of Actively adjusting Data regularly Several iterations of AND PRACTICE assessment process basic process or tools shared and discussed assessment cycle; begun; may be in the after on iteration/ through more than process is structurally pilot stage; may not pilot; some sharing one assessment driven with wide yet have data or data and discussion of data; cycle; results used to participation; may not yet be shared developing system of improved and validate process and tools are or discussed participation student learning; use established but also of results is being responsive to changing regularly documented needs in the program; system is cyclic and used to improve and validate student 11 http://facsen.wsu.edu/eppm/AssessmentStudentLearning.pdf 12 http://oai.wsu.edu/assessment_resources/assessment_highlights_long.html#assessment_data_for_change 13 http://oai.wsu.edu/assessment_resources/assessment_highlights_long.html#internship_and_field_experience 14 http://oai.wsu.edu/assessment_resources/assessment_highlights_long.html#professional_skills 15 http://accreditation.wsu.edu/reports/WSU-2013-Year-Three-Self-Evaluation-Report-Final.pdf p. 223 16 http://president.wsu.edu/blog/perspectives/2013/10/17/accreditation-report-praises-assessment-progress-outlines-need-for-more/ 27 Section 2 A S S E S S E ME N T P R A C T IC E S Summary In this section, we present a variety of assessment tasks that could be selected and organized within a framework to assess learning at the program level. They have been categorized by the types of learning outcomes they are best suited to assess, with the expectation that most can be adapted to assess more than one learning outcome. Each of these possible tasks was chosen because of its consistency with the principles outlined in