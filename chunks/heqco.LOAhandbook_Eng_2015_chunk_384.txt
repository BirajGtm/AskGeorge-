are specific to single assignments within learning outcomes that demonstrate learning at relational or one course and others reflect learning across courses (i.e., extended abstract levels. The later an assessment occurs within portfolios and capstone projects). Keeping the principles a program, the closer to extended abstract levels students are in mind makes decisions about appropriate assessment expected to reach. tasks much easier. Various design criteria are important. For When you are choosing assessment tasks, bear in mind the example, calling an assessment tool a “portfolio” is hardly principles introduced in Section 1. The need for validity helps enough; it must be designed with the appropriate content narrow the range of choices – ask yourself, would this particular and complexity. The portfolio task must be well designed and assessment task assess this particular outcome? For example, clearly communicated to students in an environment that validity in assessing the ability to critique arguments requires encourages and supports learning. Those same considerations students to critique arguments. Assessing authentically apply to the evaluation and grading of student work; if students requires students to critique arguments in a way that simulates are compared to their colleagues or held to unknown or the intended performance environment. unreasonable standards, results of the assessment experience will be unreliable. They should be held to well-articulated Ultimately, judgments about assessment measures should explicit standards drawn from your learning outcomes and be made by instructors and supported but not directed clearly communicated to students in advance. Rubrics are by educational developers, students, administrators, increasingly being developed by collaborations of universities representatives of relevant professions or government and colleges for use at the institutional level.17 Additionally, agencies. This is because assessments that are embedded adapting assessments to be authentic and relevant to the within a constructively aligned set of courses and curriculum discipline will be an important step following the selection have been found to be most efficient and relevant for assessing of the appropriate tasks for a framework of assessment.18 program-level outcomes (Barrie et al., 2009; Barrie et al., 2012; Rhodes, 2012; Kuh et al., 2014). Sadler (2013) cautions against The broad categories of assessments for the particular types of large-scale model assessment of separate competencies: outcomes in the following sections draw heavily from the work “Instead, the focus is on the concept of competence as the of McMichael (2009), who relied on the work of Nightingale capability to orchestrate knowledge and skill independently, et al. (1996) and Brown, Rust and Gibbs (1994). in a range of contexts, on demand and to a high level of 17 For example, visit http://www.aacu.org/value/rubrics/. 18 For example, visit http://www.itl.usyd.edu.au/projects/aaglo/pdf/AAGLO%20Summary%207%20Assessment%20tasks_Final.pdf. 29 SECITCARP TNEMSSESSA — 2 NOITCES 2.1 CRITICAL THINKING, PROBLEM- SOLVING, JUDGMENT AND INSIGHT “Critical thinking” is a broad term with diverse meaning in higher education, varying not only among disciplines but also between sub-disciplines and individuals working within them. This is true of the discipline of informal logic (the domain of critical thinking) as with any other system of organizing thought. In order to be effectively assessed, learning outcomes must define critical thinking in relation to